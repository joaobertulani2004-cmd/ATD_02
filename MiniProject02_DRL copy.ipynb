{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44e07f7f",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8d0820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a38e9c",
   "metadata": {},
   "source": [
    "## BASE SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b84c9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados serão guardados em: c:\\Users\\tomfp\\Documents\\GitHub\\ATD_02\\project_results\n"
     ]
    }
   ],
   "source": [
    "SEEDS = [42, 67, 136]\n",
    "TOTAL_TIMESTEPS = 100_000\n",
    "\n",
    "BASE_DIR = \"./project_results\"\n",
    "\n",
    "# --- DQN ---\n",
    "DQN_DIR = os.path.join(BASE_DIR, \"DQN\")\n",
    "DQN_LOGS = os.path.join(DQN_DIR, \"tensorboard\")\n",
    "os.makedirs(DQN_DIR, exist_ok=True)\n",
    "\n",
    "# --- PPO ---\n",
    "PPO_DIR = os.path.join(BASE_DIR, \"PPO\")\n",
    "PPO_LOGS = os.path.join(PPO_DIR, \"tensorboard\")\n",
    "os.makedirs(PPO_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Resultados serão guardados em: {os.path.abspath(BASE_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86948cb2",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c55111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNLoggingCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Logs epsilon, episodic reward and number of updates during DQN training.\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.epsilon_history = []\n",
    "        self.reward_history = []\n",
    "        self.update_steps = []\n",
    "        self.episode_reward = 0.0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Exploration rate (epsilon)\n",
    "        if hasattr(self.model, \"exploration_rate\"):\n",
    "            self.epsilon_history.append(self.model.exploration_rate)\n",
    "\n",
    "        # Reward\n",
    "        reward = self.locals.get(\"rewards\")\n",
    "        if reward is not None:\n",
    "            self.episode_reward += reward[0]\n",
    "\n",
    "        # End of episode\n",
    "        done = self.locals.get(\"dones\")\n",
    "        if done is not None and done[0]:\n",
    "            self.reward_history.append(self.episode_reward)\n",
    "            self.episode_reward = 0.0\n",
    "\n",
    "        # Gradient updates\n",
    "        self.update_steps.append(self.model._n_updates)\n",
    "        return True\n",
    "    \n",
    "class PPOLoggingCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Logs entropy, episodic reward, policy loss and value loss during PPO training.\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.entropy_history = []\n",
    "        self.policy_loss_history = []\n",
    "        self.value_loss_history = []\n",
    "        self.reward_history = []\n",
    "        self.episode_reward = 0.0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Reward\n",
    "        reward = self.locals.get(\"rewards\")\n",
    "        if reward is not None:\n",
    "            self.episode_reward += reward[0]\n",
    "\n",
    "        # End of episode\n",
    "        done = self.locals.get(\"dones\")\n",
    "        if done is not None and done[0]:\n",
    "            self.reward_history.append(self.episode_reward)\n",
    "            self.episode_reward = 0.0\n",
    "\n",
    "        # Entropy\n",
    "        entropy = self.locals.get(\"ent_coef\")  # coef * entropy\n",
    "        if entropy is not None:\n",
    "            self.entropy_history.append(entropy)\n",
    "\n",
    "        # Policy and value loss\n",
    "        if \"loss\" in self.locals:\n",
    "            loss = self.locals[\"loss\"]\n",
    "            # PPO returns a dict with keys: 'policy_loss', 'value_loss', 'entropy_loss' sometimes\n",
    "            # We'll try to log if available\n",
    "            if isinstance(loss, dict):\n",
    "                self.policy_loss_history.append(loss.get(\"policy_loss\", np.nan))\n",
    "                self.value_loss_history.append(loss.get(\"value_loss\", np.nan))\n",
    "\n",
    "        return True\n",
    "\n",
    "def setup(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(\"Python:\", sys.version.split()[0])\n",
    "    print(\"PyTorch:\", torch.__version__)\n",
    "    print(\"Device:\", device)\n",
    "    print(\"CUDA:\", torch.version.cuda if torch.cuda.is_available() else \"None\")\n",
    "    print(\"\")\n",
    "\n",
    "def make_env(seed: int):\n",
    "    def _init():\n",
    "        env = gym.make(\"LunarLander-v3\")\n",
    "        env.reset(seed=seed)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "\n",
    "def evaluate_agent(model, env_fn, n_episodes=20, render=False):\n",
    "    \"\"\"\n",
    "    Avalia um agente em n_episodes determinísticos.\n",
    "    Retorna rewards e comprimentos de episódios.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    lengths = []\n",
    "\n",
    "    env = env_fn()\n",
    "    for ep in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            done = terminated or truncated\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        lengths.append(steps)\n",
    "\n",
    "    env.close()\n",
    "    return np.array(rewards), np.array(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f22a5a",
   "metadata": {},
   "source": [
    "## TRAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e8b4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= SEED 42 =================\n",
      "\n",
      "Python: 3.11.14\n",
      "PyTorch: 2.8.0\n",
      "Device: cpu\n",
      "CUDA: None\n",
      "\n",
      "\n",
      ">>> Iniciando treino DQN com seed 42 <<<\n",
      "\n",
      "Using cpu device\n",
      "Logging to ./project_results\\DQN\\tensorboard\\dqn_seed_42_3\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.982    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 10123    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 382      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.964    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 4016     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 759      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.06     |\n",
      "|    n_updates        | 64       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.947    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 2946     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1122     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.33     |\n",
      "|    n_updates        | 155      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.931    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 2594     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1457     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.19     |\n",
      "|    n_updates        | 239      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.914    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 2431     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1809     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.02     |\n",
      "|    n_updates        | 327      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.895    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 2344     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2217     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.28     |\n",
      "|    n_updates        | 429      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.878    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 2272     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 2560     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.27     |\n",
      "|    n_updates        | 514      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.861    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 2212     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 2933     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.404    |\n",
      "|    n_updates        | 608      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.842    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 2191     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 3316     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.94     |\n",
      "|    n_updates        | 703      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.825    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 2174     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 3692     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.56     |\n",
      "|    n_updates        | 797      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.801    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 2146     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 4188     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.462    |\n",
      "|    n_updates        | 921      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.739    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 2066     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 5489     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.5      |\n",
      "|    n_updates        | 1247     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.716    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 2044     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 5971     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.01     |\n",
      "|    n_updates        | 1367     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.693    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 2020     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 6457     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.383    |\n",
      "|    n_updates        | 1489     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.666    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 1992     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 7036     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.52     |\n",
      "|    n_updates        | 1633     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.639    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 1960     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 7602     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.5      |\n",
      "|    n_updates        | 1775     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.595    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 1936     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 8536     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.791    |\n",
      "|    n_updates        | 2008     |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dqn_eval_results = {}\n",
    "ppo_eval_results = {}\n",
    "\n",
    "for seed in SEEDS:\n",
    "    print(f\"\\n================= SEED {seed} =================\\n\")\n",
    "    setup(seed)\n",
    "\n",
    "    # -----------------------\n",
    "    # DQN\n",
    "    # -----------------------\n",
    "    print(f\"\\n>>> Iniciando treino DQN com seed {seed} <<<\\n\")\n",
    "    dqn_env = DummyVecEnv([make_env(seed)])\n",
    "    dqn_callback = DQNLoggingCallback()\n",
    "\n",
    "    model_dqn = DQN(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=dqn_env,\n",
    "        learning_rate=1e-3,\n",
    "        buffer_size=50_000,\n",
    "        exploration_fraction=0.2,\n",
    "        batch_size=64,\n",
    "        gamma=0.99,\n",
    "        train_freq=4,\n",
    "        learning_starts=500,\n",
    "        target_update_interval=10_000,\n",
    "        verbose=1,\n",
    "        seed=seed,\n",
    "        tensorboard_log=DQN_LOGS,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    model_dqn.learn(\n",
    "        total_timesteps=TOTAL_TIMESTEPS,\n",
    "        callback=dqn_callback,\n",
    "        tb_log_name=f\"dqn_seed_{seed}\",\n",
    "        progress_bar=True\n",
    "    )\n",
    "\n",
    "    model_dqn.save(os.path.join(DQN_DIR, f\"model_dqn_seed_{seed}\"))\n",
    "    dqn_env.close()\n",
    "\n",
    "    # --- Avaliação DQN ---\n",
    "    rewards, lengths = evaluate_agent(model_dqn, make_env(seed), n_episodes=20)\n",
    "    dqn_eval_results[seed] = {\n",
    "        \"mean_reward\": rewards.mean(),\n",
    "        \"std_reward\": rewards.std(),\n",
    "        \"mean_length\": lengths.mean(),\n",
    "        \"std_length\": lengths.std()\n",
    "    }\n",
    "    print(f\"DQN Seed {seed}: Mean Reward = {rewards.mean():.2f} ± {rewards.std():.2f}, \"\n",
    "          f\"Mean Length = {lengths.mean():.1f} ± {lengths.std():.1f}\")\n",
    "\n",
    "    # -----------------------\n",
    "    # PPO\n",
    "    # -----------------------\n",
    "    print(f\"\\n>>> Iniciando treino PPO com seed {seed} <<<\\n\")\n",
    "    ppo_env = DummyVecEnv([make_env(seed)])\n",
    "    ppo_callback = PPOLoggingCallback()\n",
    "\n",
    "    model_ppo = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=ppo_env,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=2048,\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        ent_coef=0.01,\n",
    "        verbose=1,\n",
    "        seed=seed,\n",
    "        tensorboard_log=PPO_LOGS,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    model_ppo.learn(\n",
    "        total_timesteps=TOTAL_TIMESTEPS,\n",
    "        tb_log_name=f\"ppo_seed_{seed}\",\n",
    "        callback=ppo_callback,\n",
    "        progress_bar=True\n",
    "    )\n",
    "\n",
    "    model_ppo.save(os.path.join(PPO_DIR, f\"model_ppo_seed_{seed}\"))\n",
    "    ppo_env.close()\n",
    "\n",
    "print(\"\\n✅ Treino finalizado para DQN e PPO em 3 seeds independentes!\")\n",
    "\n",
    "# Salvar resultados DQN\n",
    "with open(os.path.join(DQN_DIR, \"eval_results.json\"), \"w\") as f:\n",
    "    json.dump(dqn_eval_results, f, indent=4)\n",
    "\n",
    "# Salvar resultados PPO\n",
    "with open(os.path.join(PPO_DIR, \"eval_results.json\"), \"w\") as f:\n",
    "    json.dump(ppo_eval_results, f, indent=4)\n",
    "\n",
    "print(\"✅ Resultados de avaliação guardados em JSON\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
